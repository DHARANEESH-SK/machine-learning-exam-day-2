# -*- coding: utf-8 -*-
"""machine learning exam day2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LGOoSZwJQDgzMZmidQlCu4t5722RVINt
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("/content/drive/MyDrive/admission.csv")

data=data.drop('Serial No.',axis=1)

data

"""Q.1 Perform Exploratory Data Analysis (EDA) tasks 

a) Visualize the 10 random rows of the data set
"""

data.sample(n=10 , replace = True)

"""b) Generate the description for numeric variables"""

data.describe()

"""c) Check the shape of the data set"""

data.shape

"""d) Generate the correlation matrix"""

sns.heatmap(data.corr(),annot=True)

"""e) Generate a correlogram"""

corr = np.corrcoef(data)
print(corr)

"""Q.2 Find out the minimum and maximum values for GRE score"""

gre = data['GRE Score'].values

print('Maximum of GRE Score is: ',data['GRE Score'].max())
print('Minimum of GRE Score is: ',data['GRE Score'].min())

"""Q.3	Find out the percentage of universities for each university rating"""

data['percentage']=data['University Rating']/data['University Rating'].sum()*100

data

"""Q.4 Convert the target variable “Chance of Admit” to categorical having values 0 and 1,such that :

Students having the “Chance of Admit” value > 0.80, are assigned value 1, and
Students having the “Chance of Admit” value < 0.80, are assigned value 0
Where 0: Low chance of Admission and 1: High chance of admission
"""

chance=data['Chance of Admit '].values

for i in range(0,len(chance)):
  if(chance[i]>0.80):
    chance[i]=1
  elif(chance[i]<=0.80):
    chance[i]=0

print(chance)

data

"""Q.5 Build a Decision Tree classifier, to predict whether a student has a low or high chance of admission to a chosen university. Perform Hyperparameter Tuning to improve the accuracy of the model."""

data.info()

x=data.drop(['Chance of Admit '],axis=1).values
y=data['Chance of Admit '].values

from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier
model=DecisionTreeClassifier()

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.25,random_state=0)

model.fit(x_train,y_train)

dtc=model.predict(xtest)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

accuracy_score(y_test,dtc)

confusion_matrix(y_test,dtc)

decision=classification_report(y_test,dtc)
print(decision)

from sklearn import decomposition, datasets
from sklearn import tree
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler

pca=decomposition.PCA()
sc=StandardScaler()

d=tree.DecisionTreeClassifier()

pipe=Pipeline(steps=[('sc',sc),('pca',pca),('d',d)])

n_components=list(range(1,x.shape[1]+1,1))

criterion=['gini','entropy']
max_depth=[2,3,4,5]

p=dict(pca__n_components=n_components,d__criterion=criterion,d__max_depth=max_depth)

clf=GridSearchCV(pipe,p)

clf.fit(x,y)

print('Best max_depth: ',clf.best_estimator_.get_params()['d__max_depth'])

print('Best Criterion: ',clf.best_estimator_.get_params()['d__criterion'])

print(clf.best_estimator_.get_params()['d'])

CV_result=cross_val_score(clf,x,y,cv=4, n_jobs=-1)

print(CV_result)

print(CV_result.mean())

print(CV_result.std())

"""Q.6 Build a Random Forest classifier, to predict whether a student has a low or high chance of admission to a chosen university."""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score

model1=RandomForestClassifier()

model1.fit(x_train,y_train)

pre=model1.predict(x_test)

pre

model1.predict_proba(x_test)

accuracy_score(y_test,pre)

confusion_matrix(y_test,pre)

rf=classification_report(y_test,pre)
print(rf)

from sklearn.metrics import roc_auc_score,auc,roc_curve

y_proba=model1.predict_proba(x_test)
y_predicted=y_proba[:,1]

print(y_predicted)

fpr,tpr,thresholds=roc_curve(y_test,y_predicted)

roc_auc=auc(fpr,tpr)
print(roc_auc)

import matplotlib.pyplot as plt

plt.figure()
plt.plot(fpr,tpr,color='red',label='ROC'%roc_auc)
plt.plot([0,1],[0,1],color='green',linestyle='--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.xlabel('False Positive Rate 1-(specificity)')
plt.ylabel('True Positive Rate(sensitivity')

"""Q.7 Also use Ensemble Modelling techniques, to predict whether a student has a low or high chance of admission to a chosen university"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

log =  LogisticRegression()
svm = SVC()
sgd = SGDClassifier()

def classifiers(clf1, clf2, clf3, x_train,y_train):
    clfs = [clf1, clf2, clf3]
    all_clfs_acc = []
    for clf in clfs:
        clf.fit(x_train, y_train)
        preds = clf.predict(x_train)
        acc = accuracy_score(y_train,preds)
        acc = acc.tolist()
        all_clfs_acc.append(acc)
    return all_clfs_acc

import warnings
warnings.filterwarnings('ignore')

classifiers(log,svm, sgd, x_train, y_train)

from sklearn.ensemble import VotingClassifier

vote= VotingClassifier(
    
    estimators=[('log_reg', log),
                ('svc', svm),
                ('sgd', sgd)], 
    voting='hard')

vote.fit(x_train, y_train)

"""Q.8 Compare all of the models and justify your choice about the optimum model"""

sns.countplot()

